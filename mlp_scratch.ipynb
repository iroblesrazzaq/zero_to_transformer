{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation from Scratch\n",
    "# Zero to Transformer - Advanced Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for neural network layers\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"Initialize a layer with random weights and zero biases\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of the input to this layer\n",
    "            output_size (int): Size of the output from this layer\n",
    "        \"\"\"\n",
    "        # TODO: Initialize weights and biases\n",
    "        # Use He initialization for weights: np.random.randn(...) * np.sqrt(2/input_size)\n",
    "        # Initialize biases as zeros\n",
    "        self.weights = None  # YOUR CODE HERE\n",
    "        self.biases = None   # YOUR CODE HERE\n",
    "        \n",
    "        # For storing values needed in backpropagation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "        # For storing gradients\n",
    "        self.dweights = None\n",
    "        self.dbiases = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass for this layer\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Inputs to this layer, shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Outputs from this layer, shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # 1. Store the input for later use in backpropagation\n",
    "        # 2. Compute the linear transformation: y = x * W + b\n",
    "        # 3. Return the result\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with actual output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass for this layer\n",
    "        \n",
    "        Args:\n",
    "            dvalues (numpy.ndarray): Gradient of the loss with respect to the output of this layer\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input of this layer\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass\n",
    "        # 1. Compute gradient with respect to weights: dL/dW = x^T * dL/dy\n",
    "        # 2. Compute gradient with respect to biases: dL/db = sum(dL/dy, axis=0)\n",
    "        # 3. Compute gradient with respect to inputs: dL/dx = dL/dy * W^T\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with gradient with respect to inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"ReLU activation function layer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize ReLU layer\"\"\"\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply ReLU activation: max(0, x)\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input values\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Outputs after applying ReLU\n",
    "        \"\"\"\n",
    "        # TODO: Implement ReLU forward pass\n",
    "        # 1. Store inputs for backpropagation\n",
    "        # 2. Apply the ReLU function\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with actual output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass for ReLU activation\n",
    "        \n",
    "        Args:\n",
    "            dvalues (numpy.ndarray): Gradient of the loss with respect to the output\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        # TODO: Implement ReLU backward pass\n",
    "        # Derivative of ReLU is 1 for inputs > 0, 0 otherwise\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with gradient with respect to inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"Softmax activation for output layer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Softmax layer\"\"\"\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply Softmax activation\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input values\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Probability distribution (sums to 1 along axis 1)\n",
    "        \"\"\"\n",
    "        # TODO: Implement a numerically stable Softmax\n",
    "        # 1. Subtract max value from each sample for numerical stability\n",
    "        # 2. Calculate exponentials (e^x) of inputs\n",
    "        # 3. Normalize by dividing by sum of exponentials\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with actual output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass for Softmax activation\n",
    "        \n",
    "        Args:\n",
    "            dvalues (numpy.ndarray): Gradient of the loss with respect to the output\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        # Note: For this assignment, we'll handle Softmax + Cross-Entropy together \n",
    "        # in the loss function for simplicity\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    \"\"\"Cross-entropy loss for classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Cross-Entropy loss\"\"\"\n",
    "        self.output = None\n",
    "        self.y_true = None\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Compute cross-entropy loss\n",
    "        \n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted probabilities from Softmax, shape (batch_size, num_classes)\n",
    "            y_true (numpy.ndarray): Ground truth values (either as indices or one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            float: Loss value\n",
    "        \"\"\"\n",
    "        # TODO: Implement cross-entropy loss\n",
    "        # 1. Convert y_true to one-hot if it's not already\n",
    "        # 2. Clip y_pred to avoid log(0) errors\n",
    "        # 3. Calculate cross-entropy loss: -sum(y_true * log(y_pred))\n",
    "        # 4. Average over the batch\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with actual loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward pass for Cross-Entropy loss\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input (y_pred)\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass for Cross-Entropy\n",
    "        # For Softmax + Cross-Entropy, the gradient is simply (y_pred - y_true)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Neural network with arbitrary layer structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize an empty neural network\"\"\"\n",
    "        self.layers = []\n",
    "        self.loss_function = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add a layer to the network\n",
    "        \n",
    "        Args:\n",
    "            layer: Layer object to add to the network\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def set_loss(self, loss_function):\n",
    "        \"\"\"Set the loss function for the network\n",
    "        \n",
    "        Args:\n",
    "            loss_function: Loss function object\n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the entire network\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data, shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Output predictions\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass through all layers\n",
    "        # Pass the input through each layer in sequence\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with output from final layer\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"Backward pass through the entire network\n",
    "        \n",
    "        Args:\n",
    "            y_true (numpy.ndarray): Ground truth values\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass through all layers\n",
    "        # 1. Start with gradient from loss function\n",
    "        # 2. Pass gradient backward through each layer in reverse order\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def train_step(self, X, y, learning_rate):\n",
    "        \"\"\"Perform one training step (forward, backward, update)\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data for this batch\n",
    "            y (numpy.ndarray): Ground truth for this batch\n",
    "            learning_rate (float): Learning rate for parameter updates\n",
    "            \n",
    "        Returns:\n",
    "            float: Loss value for this batch\n",
    "        \"\"\"\n",
    "        # TODO: Implement a single training step\n",
    "        # 1. Perform forward pass\n",
    "        # 2. Calculate loss\n",
    "        # 3. Perform backward pass\n",
    "        # 4. Update parameters\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with loss value\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size, learning_rate, X_val=None, y_val=None):\n",
    "        \"\"\"Train the network\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Training data\n",
    "            y (numpy.ndarray): Training labels\n",
    "            epochs (int): Number of epochs to train for\n",
    "            batch_size (int): Size of each mini-batch\n",
    "            learning_rate (float): Learning rate for parameter updates\n",
    "            X_val (numpy.ndarray, optional): Validation data\n",
    "            y_val (numpy.ndarray, optional): Validation labels\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history (loss, accuracy, etc.)\n",
    "        \"\"\"\n",
    "        # TODO: Implement training loop\n",
    "        # 1. Loop over epochs\n",
    "        # 2. Shuffle data for each epoch\n",
    "        # 3. Split data into mini-batches\n",
    "        # 4. Perform training step for each batch\n",
    "        # 5. Optionally perform validation after each epoch\n",
    "        # 6. Record metrics for plotting\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return {}  # Replace with training history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for input data\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted class indices\n",
    "        \"\"\"\n",
    "        # TODO: Implement prediction\n",
    "        # 1. Perform forward pass\n",
    "        # 2. Take argmax to get predicted class\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return None  # Replace with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\"Load MNIST dataset\n",
    "    \n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Load data from sklearn-compatible source\n",
    "    mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    \"\"\"Plot the loss and accuracy curves\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Training history containing loss and accuracy values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    if 'val_loss' in history:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history:\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_weights(model, input_shape=(28, 28)):\n",
    "    \"\"\"Visualize the weights of the first layer\n",
    "    \n",
    "    Args:\n",
    "        model (NeuralNetwork): Trained neural network\n",
    "        input_shape (tuple): Shape of input images (height, width)\n",
    "    \"\"\"\n",
    "    # Extract weights from the first layer\n",
    "    weights = model.layers[0].weights\n",
    "    \n",
    "    # Plot weights\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    num_neurons = min(weights.shape[0], 25)  # Show at most 25 neurons\n",
    "    grid_size = int(np.ceil(np.sqrt(num_neurons)))\n",
    "    \n",
    "    for i in range(num_neurons):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        \n",
    "        # Reshape weights to input shape\n",
    "        weight_img = weights[i].reshape(input_shape)\n",
    "        \n",
    "        plt.imshow(weight_img, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Neuron {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_mnist()\n",
    "\n",
    "X_sample, y_sample = X_train[:1000], y_train[:1000]\n",
    "X_val_sample, y_val_sample = X_test[:200], y_test[:200]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Layer(784, 128))  # Input layer -> Hidden layer\n",
    "model.add(ReLU())           # ReLU activation\n",
    "model.add(Layer(128, 64))   # Hidden layer -> Hidden layer\n",
    "model.add(ReLU())           # ReLU activation\n",
    "model.add(Layer(64, 10))    # Hidden layer -> Output layer\n",
    "model.add(Softmax())        # Softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_loss(CrossEntropyLoss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(\n",
    "    X_sample, y_sample,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    X_val=X_val_sample,\n",
    "    y_val=y_val_sample\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)\n",
    "\n",
    "# Visualize weights\n",
    "visualize_weights(model)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
