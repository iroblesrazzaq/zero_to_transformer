{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for neural network layers\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"Initialize a layer with random weights and zero biases\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of the input to this layer\n",
    "            output_size (int): Size of the output from this layer\n",
    "        \"\"\"\n",
    "        # He initialization for weights\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2/input_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        \n",
    "        # For storing values needed in backpropagation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "        # For storing gradients\n",
    "        self.dweights = None\n",
    "        self.dbiases = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass for this layer\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Inputs to this layer, shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Outputs from this layer, shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Store the input for later use in backpropagation\n",
    "        self.input = inputs\n",
    "        \n",
    "        # Compute the linear transformation: y = x * W + b\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass for this layer\n",
    "        \n",
    "        Args:\n",
    "            dvalues (numpy.ndarray): Gradient of the loss with respect to the output of this layer\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input of this layer\n",
    "        \"\"\"\n",
    "        # Compute gradient with respect to weights: dL/dW = x^T * dL/dy\n",
    "        self.dweights = np.dot(self.input.T, dvalues)\n",
    "        \n",
    "        # Compute gradient with respect to biases: dL/db = sum(dL/dy, axis=0)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Compute gradient with respect to inputs: dL/dx = dL/dy * W^T\n",
    "        dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020373e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"ReLU activation function layer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize ReLU layer\"\"\"\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply ReLU activation: max(0, x)\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input values\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Outputs after applying ReLU\n",
    "        \"\"\"\n",
    "        # Store inputs for backpropagation\n",
    "        self.input = inputs\n",
    "        \n",
    "        # Apply the ReLU function\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass for ReLU activation\n",
    "        \n",
    "        Args:\n",
    "            dvalues (numpy.ndarray): Gradient of the loss with respect to the output\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        # Make a copy of the gradient\n",
    "        dinputs = dvalues.copy()\n",
    "        \n",
    "        # Apply ReLU derivative: 1 for inputs > 0, 0 otherwise\n",
    "        dinputs[self.input <= 0] = 0\n",
    "        \n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"Softmax activation for output layer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Softmax layer\"\"\"\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply Softmax activation\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input values\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Probability distribution (sums to 1 along axis 1)\n",
    "        \"\"\"\n",
    "        # Store input for backpropagation\n",
    "        self.input = inputs\n",
    "        \n",
    "        # Subtract max value from each sample for numerical stability\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize by dividing by sum of exponentials\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass for Softmax activation\n",
    "        \n",
    "        Args:\n",
    "            dvalues (numpy.ndarray): Gradient of the loss with respect to the output\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        # Note: For this assignment, we'll handle Softmax + Cross-Entropy together \n",
    "        # in the loss function for simplicity\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20328931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    \"\"\"Cross-entropy loss for classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Cross-Entropy loss\"\"\"\n",
    "        self.output = None\n",
    "        self.y_true = None\n",
    "        self.y_pred = None\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Compute cross-entropy loss\n",
    "        \n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted probabilities from Softmax, shape (batch_size, num_classes)\n",
    "            y_true (numpy.ndarray): Ground truth values (either as indices or one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            float: Loss value\n",
    "        \"\"\"\n",
    "        # Store for backward pass\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "        # Get number of samples\n",
    "        batch_size = y_pred.shape[0]\n",
    "        \n",
    "        # Convert y_true to one-hot if it's not already (if it's 1D array of class indices)\n",
    "        if len(y_true.shape) == 1 or y_true.shape[1] == 1:\n",
    "            self.y_true = np.eye(y_pred.shape[1])[y_true.reshape(-1).astype(int)]\n",
    "        else:\n",
    "            self.y_true = y_true\n",
    "            \n",
    "        # Clip y_pred to avoid log(0) errors\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        negative_log_likelihoods = -np.sum(self.y_true * np.log(y_pred_clipped), axis=1)\n",
    "        \n",
    "        # Average over the batch\n",
    "        loss = np.mean(negative_log_likelihoods)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward pass for Cross-Entropy loss\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input (y_pred)\n",
    "        \"\"\"\n",
    "        # For Softmax + Cross-Entropy, the gradient is simply (y_pred - y_true)\n",
    "        # Get number of samples\n",
    "        batch_size = self.y_pred.shape[0]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        dinputs = self.y_pred - self.y_true\n",
    "        \n",
    "        # Normalize gradient\n",
    "        dinputs = dinputs / batch_size\n",
    "        \n",
    "        return dinputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ae7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Neural network with arbitrary layer structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize an empty neural network\"\"\"\n",
    "        self.layers = []\n",
    "        self.loss_function = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add a layer to the network\n",
    "        \n",
    "        Args:\n",
    "            layer: Layer object to add to the network\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def set_loss(self, loss_function):\n",
    "        \"\"\"Set the loss function for the network\n",
    "        \n",
    "        Args:\n",
    "            loss_function: Loss function object\n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the entire network\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data, shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Output predictions\n",
    "        \"\"\"\n",
    "        # Pass the input through each layer in sequence\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"Backward pass through the entire network\n",
    "        \n",
    "        Args:\n",
    "            y_true (numpy.ndarray): Ground truth values\n",
    "        \"\"\"\n",
    "        # Start with gradient from loss function\n",
    "        grad = self.loss_function.backward()\n",
    "        \n",
    "        # Pass gradient backward through each layer in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "    \n",
    "    def train_step(self, X, y, learning_rate):\n",
    "        \"\"\"Perform one training step (forward, backward, update)\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data for this batch\n",
    "            y (numpy.ndarray): Ground truth for this batch\n",
    "            learning_rate (float): Learning rate for parameter updates\n",
    "            \n",
    "        Returns:\n",
    "            float: Loss value for this batch\n",
    "        \"\"\"\n",
    "        # Perform forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.loss_function.forward(y_pred, y)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        self.backward(y)\n",
    "        \n",
    "        # Update parameters\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'weights'):\n",
    "                layer.weights -= learning_rate * layer.dweights\n",
    "                layer.biases -= learning_rate * layer.dbiases\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size, learning_rate, X_val=None, y_val=None):\n",
    "        \"\"\"Train the network\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Training data\n",
    "            y (numpy.ndarray): Training labels\n",
    "            epochs (int): Number of epochs to train for\n",
    "            batch_size (int): Size of each mini-batch\n",
    "            learning_rate (float): Learning rate for parameter updates\n",
    "            X_val (numpy.ndarray, optional): Validation data\n",
    "            y_val (numpy.ndarray, optional): Validation labels\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history (loss, accuracy, etc.)\n",
    "        \"\"\"\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': []\n",
    "        }\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        if X_val is not None:\n",
    "            X_val = np.array(X_val)\n",
    "        if y_val is not None:\n",
    "            y_val = np.array(y_val)\n",
    "\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            history['val_loss'] = []\n",
    "            history['val_accuracy'] = []\n",
    "        \n",
    "        num_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data for this epoch\n",
    "            indices = np.random.permutation(num_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            # Variables to track epoch statistics\n",
    "            epoch_loss = 0\n",
    "            y_pred_all = []\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                # Get mini-batch\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Perform training step\n",
    "                batch_loss = self.train_step(X_batch, y_batch, learning_rate)\n",
    "                epoch_loss += batch_loss * X_batch.shape[0]\n",
    "                \n",
    "                # Store predictions for accuracy calculation\n",
    "                y_pred_batch = self.predict(X_batch)\n",
    "                y_pred_all.extend(y_pred_batch)\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            epoch_loss /= num_samples\n",
    "            y_pred_all = np.array(y_pred_all)\n",
    "            accuracy = accuracy_score(y_shuffled[:len(y_pred_all)], y_pred_all)\n",
    "            \n",
    "            # Store metrics\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            history['train_accuracy'].append(accuracy)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\", end=\"\")\n",
    "            \n",
    "            # Validation if data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get predictions and loss on validation set\n",
    "                y_val_pred = self.forward(X_val)\n",
    "                val_loss = self.loss_function.forward(y_val_pred, y_val)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                val_accuracy = accuracy_score(y_val, self.predict(X_val))\n",
    "                \n",
    "                # Store metrics\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_accuracy'].append(val_accuracy)\n",
    "                \n",
    "                print(f\", Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\", end=\"\")\n",
    "            \n",
    "            print()  # New line\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for input data\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted class indices\n",
    "        \"\"\"\n",
    "        # Perform forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # Take argmax to get predicted class\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def load_mnist():\n",
    "    \"\"\"Load MNIST dataset\n",
    "    \n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Load data from sklearn-compatible source\n",
    "    mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    \"\"\"Plot the loss and accuracy curves\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Training history containing loss and accuracy values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    if 'val_loss' in history:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history:\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_weights(model, input_shape=(28, 28)):\n",
    "    \"\"\"Visualize the weights of the first layer\n",
    "    \n",
    "    Args:\n",
    "        model (NeuralNetwork): Trained neural network\n",
    "        input_shape (tuple): Shape of input images (height, width)\n",
    "    \"\"\"\n",
    "    # Extract weights from the first layer\n",
    "    weights = model.layers[0].weights\n",
    "    \n",
    "    # Plot weights\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    num_neurons = min(weights.shape[1], 25)  # Show at most 25 neurons\n",
    "    grid_size = int(np.ceil(np.sqrt(num_neurons)))\n",
    "    \n",
    "    for i in range(num_neurons):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        \n",
    "        # Reshape weights to input shape\n",
    "        weight_img = weights[:, i].reshape(input_shape)\n",
    "        \n",
    "        plt.imshow(weight_img, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Neuron {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\"Load MNIST dataset\n",
    "    \n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Load data from sklearn-compatible source\n",
    "    mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    \"\"\"Plot the loss and accuracy curves\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Training history containing loss and accuracy values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    if 'val_loss' in history:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history:\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_weights(model, input_shape=(28, 28)):\n",
    "    \"\"\"Visualize the weights of the first layer\n",
    "    \n",
    "    Args:\n",
    "        model (NeuralNetwork): Trained neural network\n",
    "        input_shape (tuple): Shape of input images (height, width)\n",
    "    \"\"\"\n",
    "    # Extract weights from the first layer\n",
    "    weights = model.layers[0].weights\n",
    "    \n",
    "    # Plot weights\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    num_neurons = min(weights.shape[1], 25)  # Show at most 25 neurons\n",
    "    grid_size = int(np.ceil(np.sqrt(num_neurons)))\n",
    "    \n",
    "    for i in range(num_neurons):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        \n",
    "        # Reshape weights to input shape\n",
    "        weight_img = weights[:, i].reshape(input_shape)\n",
    "        \n",
    "        plt.imshow(weight_img, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Neuron {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84829495",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_mnist()\n",
    "\n",
    "X_sample, y_sample = X_train, y_train\n",
    "X_val_sample, y_val_sample = X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fab5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Layer(784, 128))  # Input layer -> Hidden layer\n",
    "model.add(ReLU())           # ReLU activation\n",
    "model.add(Layer(128, 64))   # Hidden layer -> Hidden layer\n",
    "model.add(ReLU())           # ReLU activation\n",
    "model.add(Layer(64, 10))    # Hidden layer -> Output layer\n",
    "model.add(Softmax())        # Softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_loss(CrossEntropyLoss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = model.train(\n",
    "    X_sample, y_sample,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    X_val=X_val_sample,\n",
    "    y_val=y_val_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe179a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
